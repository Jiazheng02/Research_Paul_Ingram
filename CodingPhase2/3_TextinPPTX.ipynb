{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "class ImageProcessor:\n",
    "    \"\"\"\n",
    "    A class to extract text boxes and relationships from images inside a PPT slide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image):\n",
    "        \"\"\"\n",
    "        Initializes the text box processor.\n",
    "        \n",
    "        :param image_path: Path to the image to process.\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "\n",
    "        # Convert to grayscale\n",
    "        self.gray = cv2.cvtColor(self.image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Initialize PaddleOCR\n",
    "        self.ocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n",
    "\n",
    "        # Extract initial text boxes\n",
    "        self.boxes, self.texts, self.scores = self._extract_text_boxes()\n",
    "\n",
    "        # Mask text regions\n",
    "        self.text_boxes = self.find_boxes()\n",
    "        self.filtered_boxes = self.filter_boxes(self.text_boxes)\n",
    "\n",
    "        # Extract text boxes and relationships\n",
    "        self.text_boxes_dict = self.get_combined_text_dict()\n",
    "        self.box_centers = self._calculate_box_centers()\n",
    "\n",
    "        self.lines = self.find_lines()\n",
    "\n",
    "        self.relationships = self.match_relationships()\n",
    "    \n",
    "    def _extract_text_boxes(self):\n",
    "            \"\"\"\n",
    "            Extract text boxes, recognized texts, and confidence scores using PaddleOCR.\n",
    "\n",
    "            :return: Tuple of (boxes, texts, scores).\n",
    "            \"\"\"\n",
    "            result = self.ocr.ocr(self.image, cls=True)\n",
    "            \n",
    "            # Extract bounding boxes, recognized text, and confidence scores\n",
    "            boxes = [line[0] for line in result[0]]  # Detected text regions\n",
    "            texts = [line[1][0] for line in result[0]]  # Recognized texts\n",
    "            scores = [line[1][1] for line in result[0]]  # Confidence scores\n",
    "\n",
    "            return boxes, texts, scores\n",
    "\n",
    "    def _boxes_distance(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Compute the Euclidean distance between the centers of two text boxes.\n",
    "\n",
    "        :param box1: Coordinates of the first text box.\n",
    "        :param box2: Coordinates of the second text box.\n",
    "        :return: Euclidean distance between the two box centers.\n",
    "        \"\"\"\n",
    "        center1 = np.mean(box1, axis=0)  # Center of first box\n",
    "        center2 = np.mean(box2, axis=0)  # Center of second box\n",
    "\n",
    "        return np.linalg.norm(center1 - center2)  # Euclidean distance\n",
    "\n",
    "    # Get Text Dictionary\n",
    "    def get_combined_text_dict(self):\n",
    "        \"\"\"\n",
    "        Merges nearby text boxes and returns a dictionary mapping coordinates to merged text.\n",
    "\n",
    "        :return: Dictionary {tuple(box_coordinates): merged_text}\n",
    "        \"\"\"\n",
    "        distance_threshold = 50  # Maximum distance between two text boxes to be merged\n",
    "        combined_dict = {}  # Store box coordinates and text mapping\n",
    "        used = [False] * len(self.boxes)\n",
    "\n",
    "        for i in range(len(self.boxes)):\n",
    "            if used[i]:\n",
    "                continue  # Skip already merged boxes\n",
    "\n",
    "            # Initialize current box and text\n",
    "            current_box = np.array(self.boxes[i], dtype=np.float32)\n",
    "            current_text = [self.texts[i]]  # Store text as a list to append later\n",
    "            used[i] = True  # Mark as merged\n",
    "\n",
    "            for j in range(i + 1, len(self.boxes)):\n",
    "                if used[j]:\n",
    "                    continue\n",
    "\n",
    "                # Compute the distance between boxes\n",
    "                distance = self._boxes_distance(current_box, self.boxes[j])\n",
    "\n",
    "                # Merge if within threshold\n",
    "                if distance < distance_threshold:\n",
    "                    # Combine box coordinates\n",
    "                    combined_points = np.vstack((current_box, self.boxes[j]))\n",
    "                    combined_points = np.array(combined_points, dtype=np.float32)\n",
    "\n",
    "                    # Ensure valid bounding box\n",
    "                    if len(combined_points) >= 2:\n",
    "                        try:\n",
    "                            rect = cv2.minAreaRect(combined_points)\n",
    "                            current_box = cv2.boxPoints(rect)  # Get the updated box\n",
    "\n",
    "                            # Merge text\n",
    "                            current_text.append(self.texts[j])  # Append the text\n",
    "                            used[j] = True\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in cv2.minAreaRect: {e}\")\n",
    "                            continue\n",
    "\n",
    "            # Convert NumPy array to a tuple (so it can be used as a dictionary key)\n",
    "            box_tuple = tuple(map(tuple, current_box))  # Convert to ((x1, y1), (x2, y2), ...)\n",
    "            combined_dict[box_tuple] = \" \".join(current_text)  # Store as key-value pair\n",
    "\n",
    "        return combined_dict\n",
    "    \n",
    "    # Get All Possible Text Boxes\n",
    "    def find_boxes(self):\n",
    "        \"\"\"\n",
    "        Detects bounding boxes of potential text areas in the image.\n",
    "\n",
    "        :return: A list of bounding boxes in the format [(x1, y1, x2, y2), ...]\n",
    "        \"\"\"\n",
    "        # Convert to binary threshold (inverse)\n",
    "        _, binary = cv2.threshold(self.gray, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Find contours in the binary image\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Apply morphological operations to clean noise\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)  # Remove noise\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)  # Fill gaps\n",
    "\n",
    "        # Detect bounding boxes\n",
    "        text_boxes = []\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            if w * h > 1000:  # Filter out small regions\n",
    "                text_boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "        return text_boxes\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Computes the Intersection over Union (IoU) between two bounding boxes.\n",
    "\n",
    "        :param box1: First bounding box (x1, y1, x2, y2)\n",
    "        :param box2: Second bounding box (x1, y1, x2, y2)\n",
    "        :return: IoU score (value between 0 and 1)\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = box1\n",
    "        x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "        inter_x1 = max(x1, x1_)\n",
    "        inter_y1 = max(y1, y1_)\n",
    "        inter_x2 = min(x2, x2_)\n",
    "        inter_y2 = min(y2, y2_)\n",
    "\n",
    "        inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "        box1_area = (x2 - x1) * (y2 - y1)\n",
    "        box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "        return inter_area / float(box1_area + box2_area - inter_area)\n",
    "\n",
    "    def filter_boxes(self, text_boxes):\n",
    "        \"\"\"\n",
    "        Filters out overlapping bounding boxes.\n",
    "\n",
    "        :param text_boxes: List of bounding boxes.\n",
    "        :return: Filtered list of bounding boxes.\n",
    "        \"\"\"\n",
    "        filtered_boxes = []\n",
    "        for i, box1 in enumerate(text_boxes):\n",
    "            keep = True\n",
    "            for j, box2 in enumerate(text_boxes):\n",
    "                if i != j and self.iou(box1, box2) > 0.4:  # Overlapping threshold\n",
    "                    if (box1[2] - box1[0]) * (box1[3] - box1[1]) > (box2[2] - box2[0]) * (box2[3] - box2[1]):\n",
    "                        keep = False  # Remove the larger box if overlap is high\n",
    "            if keep:\n",
    "                filtered_boxes.append(box1)\n",
    "\n",
    "        return [box for box in filtered_boxes if not (box[0] == 0 and box[1] == 0)]\n",
    "\n",
    "    # Filter out Detected Boxes to Get Lines\n",
    "    def apply_mask(self):\n",
    "        \"\"\"\n",
    "        Applies a mask to remove detected text areas from the image.\n",
    "        \n",
    "        :return: Masked grayscale image and its edge-detected version.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(self.gray) * 255\n",
    "        for box in self.filtered_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            mask[y1:y2, x1:x2] = 0  # Mask out text areas\n",
    "\n",
    "        return cv2.bitwise_and(self.gray, self.gray, mask=mask), cv2.Canny(mask, 100, 200)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_point_near_edge(x, y, mask_edges, neighborhood_size=5):\n",
    "        \"\"\"\n",
    "        Checks if a point (x, y) is near an edge in the masked image.\n",
    "\n",
    "        :param x: X-coordinate of the point.\n",
    "        :param y: Y-coordinate of the point.\n",
    "        :param mask_edges: Edge-detected version of the masked image.\n",
    "        :param neighborhood_size: Search window size.\n",
    "        :return: Boolean indicating whether the point is near an edge.\n",
    "        \"\"\"\n",
    "        x_min = max(0, x - neighborhood_size // 2)\n",
    "        x_max = min(mask_edges.shape[1] - 1, x + neighborhood_size // 2)\n",
    "        y_min = max(0, y - neighborhood_size // 2)\n",
    "        y_max = min(mask_edges.shape[0] - 1, y + neighborhood_size // 2)\n",
    "\n",
    "        return np.any(mask_edges[y_min:y_max + 1, x_min:x_max + 1] == 255)\n",
    "\n",
    "    @staticmethod\n",
    "    def are_lines_similar(line1, line2, length_threshold=10, midpoint_threshold=10, angle_threshold=10):\n",
    "        \"\"\"\n",
    "        Determines whether two lines are similar based on length, midpoint, and angle.\n",
    "\n",
    "        :param line1: First line segment (x1, y1, x2, y2)\n",
    "        :param line2: Second line segment (x1, y1, x2, y2)\n",
    "        :return: Boolean indicating similarity.\n",
    "        \"\"\"\n",
    "        def normalize_line(line):\n",
    "            x0, y0, x1, y1 = line\n",
    "            return (x0, y0, x1, y1) if x0 < x1 else (x1, y1, x0, y0)\n",
    "\n",
    "        line1, line2 = normalize_line(line1), normalize_line(line2)\n",
    "\n",
    "        def line_length(line):\n",
    "            return np.linalg.norm([line[2] - line[0], line[3] - line[1]])\n",
    "\n",
    "        def line_midpoint(line):\n",
    "            return ((line[0] + line[2]) / 2, (line[1] + line[3]) / 2)\n",
    "\n",
    "        def line_angle(line):\n",
    "            dx, dy = line[2] - line[0], line[3] - line[1]\n",
    "            return np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "        length_diff = abs(line_length(line1) - line_length(line2))\n",
    "        midpoint_dist = np.linalg.norm(np.array(line_midpoint(line1)) - np.array(line_midpoint(line2)))\n",
    "        angle_diff = abs(line_angle(line1) - line_angle(line2))\n",
    "        angle_diff = min(angle_diff, 360 - angle_diff)  # Handle circular angle differences\n",
    "\n",
    "        return (length_diff < length_threshold and midpoint_dist < midpoint_threshold and angle_diff < angle_threshold)\n",
    "    \n",
    "    def find_lines(self):\n",
    "        \"\"\"\n",
    "        Detects unique line segments in the image, filtering out redundant lines.\n",
    "\n",
    "        :return: List of detected lines in the format [(x1, y1, x2, y2), ...]\n",
    "        \"\"\"\n",
    "        masked_gray, mask_edges = self.apply_mask()\n",
    "\n",
    "        # Detect lines using Line Segment Detector\n",
    "        lsd = cv2.createLineSegmentDetector(0)\n",
    "        dlines = lsd.detect(masked_gray)\n",
    "\n",
    "        unique_lines = []\n",
    "\n",
    "        if dlines is not None:\n",
    "            for dline in dlines[0]:\n",
    "                x0, y0, x1, y1 = map(int, dline[0])\n",
    "\n",
    "                if self.is_point_near_edge(x0, y0, mask_edges) and self.is_point_near_edge(x1, y1, mask_edges):\n",
    "                    continue  # Ignore lines near text regions\n",
    "\n",
    "                if np.linalg.norm([x1 - x0, y1 - y0]) > 10:  # Minimum line length\n",
    "                    current_line = (x0, y0, x1, y1)\n",
    "\n",
    "                    if not any(self.are_lines_similar(current_line, line) for line in unique_lines):\n",
    "                        unique_lines.append(current_line)\n",
    "\n",
    "        return unique_lines\n",
    "\n",
    "    def _calculate_box_centers(self):\n",
    "            \"\"\"\n",
    "            Computes the center points for all text boxes.\n",
    "\n",
    "            :return: Dictionary {(cx, cy): text}\n",
    "                    - Key: Center coordinates (cx, cy)\n",
    "                    - Value: Corresponding text inside the bounding box\n",
    "            \"\"\"\n",
    "            centers_dict = {}\n",
    "            for box, text in self.text_boxes_dict.items():\n",
    "                # Compute the center of the bounding box\n",
    "                cx = np.mean([p[0] for p in box])  # Average x-coordinates\n",
    "                cy = np.mean([p[1] for p in box])  # Average y-coordinates\n",
    "                centers_dict[(cx, cy)] = text  # Store as dictionary {center: text}\n",
    "            return centers_dict\n",
    "\n",
    "    def _find_nearest_textbox(self, x, y):\n",
    "        \"\"\"\n",
    "        Finds the nearest text box center for a given point (x, y).\n",
    "\n",
    "        :param x: X-coordinate of the point\n",
    "        :param y: Y-coordinate of the point\n",
    "        :return: The text of the nearest box and its center (cx, cy),\n",
    "                 or None if no match is found\n",
    "        \"\"\"\n",
    "        min_distance = float(\"inf\")\n",
    "        nearest_text = None\n",
    "        nearest_center = None  # Store the nearest box center\n",
    "\n",
    "        for (cx, cy), text in self.box_centers.items():\n",
    "            distance = np.sqrt((cx - x) ** 2 + (cy - y) ** 2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_text = text\n",
    "                nearest_center = (cx, cy)  # Save the nearest center\n",
    "\n",
    "        return nearest_text, nearest_center\n",
    "\n",
    "    def match_relationships(self):\n",
    "        \"\"\"\n",
    "        Matches text boxes based on line segment connections (from-bottom-to-top)\n",
    "        and removes duplicates while ensuring correct ordering.\n",
    "\n",
    "        :return: List of unique (from_text, to_text) relationships sorted bottom-to-top, left-to-right.\n",
    "        \"\"\"\n",
    "        relationships = set()  # Use a set to remove duplicates\n",
    "\n",
    "        for line in self.lines:\n",
    "            x0, y0, x1, y1 = line  # Extract line segment endpoints\n",
    "\n",
    "            # Find the nearest text box for both endpoints\n",
    "            text_from, center_from = self._find_nearest_textbox(x0, y0)\n",
    "            text_to, center_to = self._find_nearest_textbox(x1, y1)\n",
    "\n",
    "            # Ensure valid matches and maintain \"bottom-to-top\" relationship\n",
    "            if text_from and text_to and (text_from != text_to):\n",
    "                if y0 > y1:  # Ensure \"from\" is at the bottom\n",
    "                    relationships.add((text_from, text_to, center_from, center_to))\n",
    "                else:  # Swap if necessary\n",
    "                    relationships.add((text_to, text_from, center_to, center_from))\n",
    "\n",
    "        # **Sorting rules**\n",
    "        # 1. Sort primarily by `from_center[1]` in descending order (bottom-to-top)\n",
    "        # 2. If `y` coordinates are the same, sort by `from_center[0]` in ascending order (left-to-right)\n",
    "        sorted_relationships = sorted(\n",
    "            relationships, key=lambda item: (-item[2][1], item[2][0])\n",
    "        )\n",
    "\n",
    "        # Return the final relationship list\n",
    "        return [(from_text, to_text) for from_text, to_text, _, _ in sorted_relationships]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"image1.png\")  # 你可以换成PPT提取的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/03/19 17:29:36] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/jiazhengtian/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/jiazhengtian/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/jiazhengtian/anaconda3/lib/python3.10/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/jiazhengtian/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2025/03/19 17:29:38] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.26687121391296387\n",
      "[2025/03/19 17:29:38] ppocr DEBUG: cls num  : 9, elapsed : 0.10591411590576172\n",
      "[2025/03/19 17:29:39] ppocr DEBUG: rec_res num  : 9, elapsed : 1.0108931064605713\n"
     ]
    }
   ],
   "source": [
    "processor = ImageProcessor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Strength', 'Discovery'),\n",
       " ('Strength', 'Bravery'),\n",
       " ('Strength', 'Ecstasy'),\n",
       " ('Strength', 'Calm'),\n",
       " ('Calm', 'Connectedto the universe'),\n",
       " ('Calm', 'Discovery'),\n",
       " ('Ecstasy', 'Connectedto the universe'),\n",
       " ('Bravery', 'Growth'),\n",
       " ('Discovery', 'Growth'),\n",
       " ('Connectedto the universe', 'Zen'),\n",
       " ('Growth', 'Zen')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.match_relationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = processor.find_lines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((274.0, 24.0), (304.0, 24.0), (304.0, 39.0), (274.0, 39.0)): 'Zen',\n",
       " ((169.0, 126.0), (226.0, 126.0), (226.0, 144.0), (169.0, 144.0)): 'Growth',\n",
       " ((341.99997, 159.0),\n",
       "  (341.99997, 119.0),\n",
       "  (466.99997, 119.0),\n",
       "  (466.99997, 159.0)): 'Connectedto the universe',\n",
       " ((30.0, 201.0), (104.0, 205.0), (103.0, 222.0), (29.0, 217.0)): 'Discovery',\n",
       " ((185.0, 213.0), (244.0, 216.0), (243.0, 234.0), (184.0, 230.0)): 'Bravery',\n",
       " ((304.0, 230.0), (359.0, 234.0), (358.0, 251.0), (302.0, 248.0)): 'Ecstasy',\n",
       " ((492.0, 243.0), (530.0, 243.0), (530.0, 258.0), (492.0, 258.0)): 'Calm',\n",
       " ((253.0, 339.0), (319.0, 339.0), (319.0, 356.0), (253.0, 356.0)): 'Strength'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_combined_text_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
